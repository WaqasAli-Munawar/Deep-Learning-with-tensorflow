{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using recurrent dropout to fight overfitting\n",
    "\n",
    "We’re already familiar with a classic technique for fighting overfitting: \n",
    "* dropout, which randomly zeros out input units of a layer in order to break happenstance correlations in the training data that the layer is exposed to. \n",
    "\n",
    "But how to correctly apply dropout in recurrent networks isn’t a trivial question. It has long been known that applying dropout before a recurrent layer hinders learning rather than helping with\n",
    "regularization. In 2015, Yarin Gal, as part of his PhD thesis on Bayesian deep learning, determined the proper way to use dropout with a recurrent network: \n",
    "* The same dropout mask (the same pattern of dropped units) should be applied at every timestep, instead of a dropout mask that varies randomly from timestep to timestep.\n",
    "\n",
    "What’s more, in order to regularize the representations formed by the recurrent gates\n",
    "of layers such as `GRU` and `LSTM`, a temporally constant dropout mask should be applied\n",
    "to the inner recurrent activations of the layer (a recurrent dropout mask). \n",
    "\n",
    "Using the same dropout mask at every timestep allows the network to properly propagate its\n",
    "learning error through time; a temporally random dropout mask would disrupt this\n",
    "error signal and be harmful to the learning process.\n",
    " \n",
    "Yarin Gal did his research using Keras and helped build this mechanism directly\n",
    "into Keras recurrent layers. Every recurrent layer in Keras has two dropout-related\n",
    "arguments: \n",
    "* `dropout`, a float specifying the dropout rate for input units of the layer, and\n",
    "* `recurrent_dropout`, specifying the dropout rate of the recurrent units. \n",
    "\n",
    "Let’s add `dropout` and `recurrent dropout` to the `GRU` layer and see how doing so impacts overfitting. Because networks being regularized with dropout always take longer to fully converge, we’ll train the network for twice as many epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.optimizers import RMSprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspecting the data of the Jena weather dataset\n",
    "\n",
    "import os\n",
    "\n",
    "data_dir = 'jena_climate'\n",
    "fname = os.path.join(data_dir, 'jena_climate_2009_2016.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(fname)\n",
    "data = f.read()\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = data.split('\\n')\n",
    "header = lines[0].split(',')\n",
    "lines = lines[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parsing the data\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "float_data = np.zeros((len(lines), len(header) - 1))\n",
    "\n",
    "for i, line in enumerate(lines):\n",
    "    values = [float(x) for x in line.split(',')[1:]]\n",
    "    float_data[i, :] = values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizing the data\n",
    "\n",
    "mean = float_data[:200000].mean(axis=0)\n",
    "float_data -= mean\n",
    "std = float_data[:200000].std(axis=0)\n",
    "float_data /= std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Training and evaluating a dropout-regularized GRU-based model\n",
    "\n",
    "model = Sequential()\n",
    "model.add(layers.GRU(32,dropout=0.2,recurrent_dropout=0.2,input_shape=(None, float_data.shape[-1])))\n",
    "model.add(layers.Dense(1))\n",
    "\n",
    "model.compile(optimizer=RMSprop(), loss='mae')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generator yielding timeseries samples and their targets\n",
    "\n",
    "def generator(data, lookback, delay, min_index, max_index,shuffle=False, batch_size=128, step=6):\n",
    "    \n",
    "    if max_index is None:\n",
    "        max_index = len(data) - delay - 1\n",
    "    \n",
    "    i = min_index + lookback\n",
    "    \n",
    "    while 1:\n",
    "        \n",
    "        if shuffle:\n",
    "            rows = np.random.randint(min_index + lookback, max_index, size=batch_size)\n",
    "        \n",
    "        else:\n",
    "            if i + batch_size >= max_index:\n",
    "                i = min_index + lookback\n",
    "            rows = np.arange(i, min(i + batch_size, max_index))\n",
    "            i += len(rows)\n",
    "        \n",
    "        samples = np.zeros((len(rows),lookback // step,data.shape[-1]))\n",
    "        targets = np.zeros((len(rows),))\n",
    "        \n",
    "        for j, row in enumerate(rows):\n",
    "            indices = range(rows[j] - lookback, rows[j], step)\n",
    "            samples[j] = data[indices]\n",
    "            targets[j] = data[rows[j] + delay][1]\n",
    "        \n",
    "        yield samples, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing the training, validation, and test generators\n",
    "\n",
    "lookback = 1440\n",
    "step = 6\n",
    "delay = 144\n",
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gen = generator(float_data,lookback=lookback,delay=delay,min_index=0,\n",
    "                      max_index=200000,shuffle=True,step=step,batch_size=batch_size)\n",
    "\n",
    "val_gen = generator(float_data,lookback=lookback,delay=delay,min_index=200001,\n",
    "                    max_index=300000,step=step,batch_size=batch_size)\n",
    "\n",
    "test_gen = generator(float_data,lookback=lookback,delay=delay,min_index=300001,\n",
    "                     max_index=None,step=step,batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many steps to draw from val_gen in order to see the entire validation set\n",
    "val_steps = (300000 - 200001 - lookback)\n",
    "\n",
    "# How many steps to draw from test_gen in order to see the entire test set \n",
    "test_steps = (len(float_data) - 300001 - lookback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit_generator(train_gen,steps_per_epoch=500,epochs=40,\n",
    "                              validation_data=val_gen,validation_steps=val_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting results\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "epochs = range(1, len(loss) + 1)\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Figure above shows the results. Success! We’re no longer overfitting during the first 30\n",
    "epochs. But although we have more stable evaluation scores, our best scores aren’t\n",
    "much lower than they were previously. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stacking Recurrent layers\n",
    "\n",
    "Because we’re no longer overfitting but seem to have hit a performance bottleneck,\n",
    "we should consider increasing the capacity of the network. Recall the description of\n",
    "the universal machine-learning workflow: it’s generally a good idea to increase the\n",
    "capacity of our network until overfitting becomes the primary obstacle (assuming we’re already taking basic steps to mitigate overfitting, such as using dropout). \n",
    "\n",
    "As long as we aren’t overfitting too badly, we’re likely under capacity. Increasing network capacity is typically done by \n",
    "* Increasing the number of units in the layers or \n",
    "* Adding more layers. \n",
    "\n",
    "Recurrent layer stacking is a classic way to build more-powerful recurrent networks: for instance, what currently powers the **Google Translate algorithm** is a **stack of seven large `LSTM` layers—that’s huge**.\n",
    "\n",
    "To stack recurrent layers on top of each other in **Tensorflow/Keras**, all intermediate layers\n",
    "should return their full sequence of outputs (a 3D tensor) rather than their output at\n",
    "the last timestep. This is done by specifying `return_sequences=True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training and evaluating a dropout-regularized, stacked GRU model\n",
    "\n",
    "model = Sequential()\n",
    "model.add(layers.GRU(32,dropout=0.1,recurrent_dropout=0.5,return_sequences=True,\n",
    "                     input_shape=(None, float_data.shape[-1])))\n",
    "model.add(layers.GRU(64, activation='relu',dropout=0.1,recurrent_dropout=0.5))\n",
    "model.add(layers.Dense(1))\n",
    "\n",
    "model.compile(optimizer=RMSprop(), loss='mae')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit_generator(train_gen,steps_per_epoch=500,epochs=40,\n",
    "                              validation_data=val_gen,validation_steps=val_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting results\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "epochs = range(1, len(loss) + 1)\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Figure above shows the results. We can see that the added layer does improve the\n",
    "results a bit, though not significantly. We can draw two conclusions:\n",
    "* Because we’re still not overfitting too badly, we could safely increase the size of our layers in a quest for validation-loss improvement. This has a non-negligible computational cost, though.\n",
    "* Adding a layer didn’t help by a significant factor, so we may be seeing diminishing returns from increasing network capacity at this point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
